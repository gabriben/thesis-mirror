%!TEX root = ../main.tex


\section{Feedback}
\label{section:feedback}

In this section, we discuss the feedback we received about the course from the perspective of participating students (Section~\ref{section:feedback-students}) and from the ML Reproducibility Challenge reviews (Section~\ref{section:feedback-mlrc}). 
 
\subsection{Feedback from Students}
\label{section:feedback-students}
Both iterations of the course were evaluated using the standard evaluation procedure provided by \OurUniversity. 
However, only 16\% of students filled out the evaluation form (23 out of 144) in the 2020--2021 iteration, potentially because the evaluation forms were administered online instead of in-person. 
According to the evaluation procedure at our university, this is not enough for a reliable quantitative estimate of student satisfaction.
Therefore, we focus on the 2019--2020 iteration when reporting student satisfaction statistics, since 53\% of students filled out the form (79 out of 149) that year. 
%
The vast majority of students were (very) satisfied with the course overall (67.8\%). 
More specifically, students expressed satisfaction with the following dimensions: 
\begin{itemize}
\setlength\itemsep{0.1em}
    \item Academic challenge: 75.2\% were (very) satisfied
    \item Supervision: 76.9\% were (very) satisfied
    \item Feedback: 81.3\% were (very) satisfied
    \item Workload: 91.3\% were (very) satisfied
    \item Level of the course: 79.7\% were (very) satisfied
    \item Level of the report: 94.8\% were (very) satisfied
    \item Level of the presentation: 96.6\% were (very) satisfied
\end{itemize}


\begin{table}[htp]
\caption{Feedback about the course.}
\centering
\begin{tabular}{@{}l@{}}
\toprule
(a) Feedback from students \\
\midrule
\begin{minipage}[t]{\columnwidth}
\begin{itemize}[leftmargin=*]
    \item ``Reproducing an article was hard and intensive but a really good experience.''
     \item ``Replicating another study, seeing how (poorly) other research is performed was really eye-opening.''
    \item ``Reproducing a paper: I believe this is a good thing to do and is an important part of academia.''
    \item  ``Gave good insights into the trustworthiness of research papers, which is apparently not great.''
    \item ``I appreciate the critical view I have developed on papers as a result of this course. Normally I would easily accept the content of a paper, but I will be more critical from now on, as many papers are not reproducible.''
    \item ``I think it's really good that we get some practical insights into reproducing results from other papers, not all papers are as good as they seem to be.''
    \item ``I really appreciated that this was the first course where students are judging state-of-the-art AI-models. In other words, students were able to experience the scientific workfield of AI.''
\end{itemize}
\end{minipage}
\\ \\
\midrule
(b) Feedback from the ML Reproducibility Challenge \\
\midrule
\begin{minipage}[t]{\columnwidth}
\begin{itemize}[leftmargin=*]
\item ``The report reveals a lot of dark spots of the original paper.''
\item ``Good reviews, strong reproducibility report, provides code reimplementation from scratch which is a strong contribution.''
\item ``The discussion section is a great reference point for future work.''
\item ``The additional experimentation is rather impressive and the report reflects  an intuitive understanding of concepts such as  coverage, correctness, and counterfactual explanations.''
\item ``The report provides good insights on how the experiments in the original paper actually work, while also generating new hypothesis to be tested for future research, which is a positive outcome.''
\item ``My main concern is that it remains unclear why some of the results are so far off from the original paper? I would have expected the authors to dig deeper on that.''
%\item ``The paper is generally difficult to follow. The paper reads closer to an outline than a finished report.''
\item ``It doesn't go above and beyond the reproduction and does not offer novel insights into the workings of the original paper.''
\item ``The submission failed at reproducing the original results. It is unclear whether this is due to a difference in the experimental setup or due to implementation errors. ''
%\item ``The paper reads closer to an outline than a finished report. I would encourage the authors to spend some additional time on organization, making sure that the key takeaways are made plain and that the report reads fluidly throughout.''
\end{itemize}
\end{minipage}
\end{tabular}
\label{tab:feedback}
\end{table}



Table~\ref{tab:feedback}(a) shows some of the qualitative feedback we received from students. 
Based on this, we believe these high scores are mostly the result of the reproducibility project. 
Students enjoyed doing the project, especially due to the intensive supervision from our experienced TAs. 
The dimensions where we received the lowest scores were on the lectures and the final presentation, where only 30.6\% and 30.2\% were (very) satisfied with these aspects, respectively. 
This may be because we only provided four (high-level) lectures on each of the four topics, in order to give students as much time as possible to focus on the reproducibility project. 
However, it should be noted that the overall scores for these components were not poor, but average: 3.1/5 for lectures and 3.0/5 for the presentation. 

\subsection{Feedback from the ML Reproducibility Challenge}
\label{section:feedback-mlrc}

Of the 30 reproducibility reports submitted to the ML Reproducibility Challenge in the 2020--2021 iteration, 9 were accepted for publication in the ReScience Journal.  
In total, the ML Reproducibility Challenge accepted 23 reports, meaning that almost 40\% of the reports accepted to the challenge were from \OurUniversity{}.\footnote{\url{https://openreview.net/group?id=ML_Reproducibility_Challenge/2020}}
 
The reviews were mostly positive, with the general consensus being that most teams had gone beyond the general expectation of simply re-implementing the algorithm and re-running the experiments. 
Our TAs encouraged students to examine the generalizability of the work that was reproduced, either by trying new datasets or hyperparameters, or by performing ablation studies. 
Multiple reproducibility reports managed to question the results of the original papers with experimentally-supported claims. 
Importantly, some reviewers emphasized that these reproducibility studies were solid starting points for future research projects. 
For the reports that were rejected, the main critiques were that
\begin{enumerate*}[label=(\roman*)]
    \item only a fraction of the original work was reproduced, or 
    \item no new insights were given.
\end{enumerate*}
Some projects also had flaws in the experimental setup. 
See Table~\ref{tab:feedback}(b) for quotes from the ML Reproducibility Challenge reviews.  










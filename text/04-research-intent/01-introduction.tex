% !TEX root = ../thesis-main.tex


\section{Introduction}
\label{section:1}
As more and more decisions about humans are made by machines, it becomes imperative to understand how these outputs are produced and what drives a model to a particular prediction \citep{riberio-2016-model}. 
As a result, algorithmic interpretability has gained significant interest and traction in the machine learning (ML) community over the past few years \citep{doshi-2017-towards}.
However, there exists considerable skepticism outside of the ML community due to a perceived lack of transparency behind algorithmic predictions, especially when errors are produced \citep{dietvorst-2015-aa}. 
We aim to evaluate the effect of explaining model outputs, specifically large errors, on users' attitudes towards trusting and deploying complex, automatically learned models. 

Further motivation for explainable ML is provided by significant societal developments. 
Important examples include the recently enacted European General Data Protection Regulation (GDPR), which specifies that individuals will have the right to ``the logic involved in any automatic personal data processing'' \citep{gdpr}. 
In Canada and the United States, this right to an explanation is an integral part of financial regulations, which is why banks have not been able to use high-performing ``black-box'' models to evaluate the credit-worthiness of their customers. 
Instead, they have been confined to easily interpretable algorithms such as decision trees (for segmenting populations) and logistic regression (for building risk scorecards) \citep{khandani-2010-consumer}. 
At NeurIPS 2017, the Explainable ML Challenge was launched to combat this limitation, indicating the finance industry's interest in exploring algorithmic explanations \citep{fico_2017}. 

We use explanations as a mechanism for supporting innovation and technological development while keeping the human ``in the loop'' by focusing on predictive modeling as a tool that aids individuals with a given task. 
Specifically, our interest lies with interpretability in a scenario where users with varying degrees of ML expertise are confronted with large errors in the outcome of predictive models. 
We focus on explaining large errors because people tend to be more curious about unexpected outcomes rather than ones that confirm their prior beliefs~\citep{hilton-1986-knowledge}. 

However, \citet{dietvorst-2015-aa} show that when users are confronted with errors in algorithmic predictions, they are less likely to use the model. 
Seeing an algorithm make mistakes significantly decreases confidence in the model, and users are more likely to choose a human forecaster instead, even after seeing the algorithm outperform the human \citep{dietvorst-2015-aa}. 
This indicates that prediction mistakes have a significant impact on users' perception of the model. 
By focusing on explaining mistakes, we hope to give insight into this phenomenon of algorithm aversion while also giving users the types of explanations they are interested in seeing. 

Our work was motivated by the needs of analysts working on sales forecasting at Ahold Delhaize, a large retailer in the Netherlands. 
Current models in production are based on simple autoregressive methods, but there is an interest in exploring more complex techniques. 
However, the added complexity comes at the expense of interpretability, which is problematic for \OurCompany{}, especially when a complex model produces a forecast that is very different from the actual target value. 
This leads us to focus on explaining errors in regression predictions in this work. 
However, it should be noted that our method can be extended to classification predictions by defining ``distances'' between classes or by simply defining all errors as large errors. 

We focus on two aspects of explainability in this scenario: the \emph{generation} of explanations of large errors and the corresponding \emph{effectiveness} of these explanations. 
Prior methods for generating explanations fail at generating explanations for large errors because they produce similar explanations for predictions resulting in large errors and those resulting in reasonable predictions (see Table~\ref{table:lime} in Section~\ref{section:4} for an example). 
We propose a method for explaining large prediction errors, called Monte Carlo Bounds for Reasonable Predictions (MC-BRP), that shows users: 
\begin{enumerate}[(i)]
\item The required bounds of the most important features in order to have a prediction resulting in a reasonable prediction.
\item The relationship between each of these features and the target.
\end{enumerate}
\pagebreak

It should be noted that in this chapter, we focus on explaining errors \emph{in hindsight}, that is, we examine large errors once they have occurred and are not predicting them in advance without having access to the ground truth. We are also not using these explanations to improve the model, but rather examine the effectiveness of explaining large errors via \OurMethod{} on users' trust in the model and attitudes towards deploying it, as well as their understanding of the explanations. 
%We examine the effectiveness of explaining large errors via \OurMethod{} through a user study aimed at determining users' understanding of the explanations as well as their trust in the model and attitudes towards deploying it based on the explanations. 
We test on a wide range of users, including both practitioners and researchers, and analyze the differences in attitudes between these users.  
We also reflect on the process of conducting a user study by outlining some limitations of our study and make some recommendations for future work. 
%
We address the following research questions: \\
\textbf{RQ3.1:} \emph{Are the contrastive explanations generated by \OurMethod{} about large errors in predictions
\begin{inparaenum}[(i)]
\item interpretable, or
\item actionable? 
\end{inparaenum}}
More specifically, 
\begin{enumerate}[(i)]
 \item Can contrastive explanations about large errors give users enough information to simulate the model's output (forward simulation)?
 \item Can such explanations help users understand the model such that they can manipulate an observation's input values in order to change the output (counterfactual simulation)?
\end{enumerate}
\textbf{RQ3.2:} \emph{How does providing contrastive explanations generated by MC-BRP for large errors impact users' perception of the model?} Specifically, we want to investigate the following:
\begin{enumerate}[(i)]
 \item Does being provided with contrastive explanations generated by MC-BRP impact users' understanding of why the model produces errors?
 \item Does it impact their willingness to deploy the model?
 \item Does it impact their level of trust in the model?
 \item Does it impact their confidence in the model's performance?
\end{enumerate}
Consequently, we make the following contributions:
\begin{itemize}
	\item We contribute a method, \OurMethod{}, for generating contrastive explanations specifically for large errors in regression tasks. 
	\item We evaluate our explanations through a user study with \numprint{75} participants in both objective and subjective terms. 
	\item We conduct an analysis on the differences in attitudes between practitioners and researchers.  
\end{itemize}

\noindent
In Section~\ref{section:2} we discuss related work and identify how our problem relates to the current literature. 
In Section~\ref{section:3} we formally describe the methodology of explanations based on \OurMethod{} and in Section~\ref{section:4} we describe our experimental setup. 
In Section~\ref{section:5} we detail the results of the user study; we conduct further analyses in Section~\ref{section:6}. 
In Section~\ref{section:7} we conclude and make recommendations for future work.


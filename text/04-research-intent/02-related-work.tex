% !TEX root = ../thesis-main.tex

\section{Related Work}
\label{section:2}
Based on the taxonomy described in Chapter~\ref{chapter:introduction}, our setting in this chapter is an \emph{local explanation} problem. Our method is model-agnostic in nature, but we evaluate it specifically on \emph{tree ensembles}. 
We use \emph{sensitivity analysis}, specifically Monte Carlo simulations, on \emph{tabular} data to generate our explanations. 

\subsection{Local Explanations for Tree Ensembles}

Existing work on generating local explanations for tree ensembles involves finding counterfactual examples \citep{tolomei_interpretable_2017}, identifying influential training samples \citep{sharchilev-2018-finding}, or identifying important features \citep{lundberg_explainable_2019}. 
Importantly, none of these publications are specifically about 
\begin{inparaenum}[(i)]
	\item explaining errors, or
	\item explaining regressions. 
\end{inparaenum} 
On the contrary, these publications are all based on binary classification tasks and the explanations do not necessarily provide insight into prediction mistakes. 

\citet{tolomei_interpretable_2017} propose a method for generating counterfactual examples by identifying decision paths of interest that would result in a different prediction, then traversing down each of these paths and perturbing the instance $x$ such that it satisfies the path in question. 
If this perturbation, $x'$,  
\begin{inparaenum}[(i)]
	\item satisfies the decision path, and
	\item changes the prediction in the overall ensemble, 
\end{inparaenum}
then it is a candidate transformation of $x$. 
After computing all possible candidate transformations by traversing over all paths of interest (i.e., those leading to a different prediction), the candidate transformation with the smallest distance from $x$ is selected as the counterfactual example. 
The explanation, then, is the difference between $x$ and $x'$. 
Although \citet{tolomei_interpretable_2017}'s method also produces contrastive explanations, our method differs from theirs since we are not aiming to identify one counterfactual example, but rather a range of feature values for which the prediction would be different. 
Another difference is that we do not assume full access to the original model. 

\citet{sharchilev-2018-finding} also generate local explanations for tree ensembles. 
Their methodology is based on finding influential training samples in order to automatically improve the model, which differs from our work since their explanations are not of a contrastive nature. 
These influential training samples help us understand why a certain class was predicted for a given instance, but they make no reference to the alternative class(es). 
It should be noted that they include a use case on identifying harmful training examples --- ones that contributed to incorrect predictions --- which can be seen as a way to explain errors. 
%\citet{koh-2017-understanding} also use influence functions to show the effect of upweighting samples or perturbing feature values on a model's parameters, but their method only applies to smooth parametric models. 

\subsection{Feature Importance Explanations}

\citet{lundberg_explainable_2019} propose a method for determining how much each feature contributes to a prediction and present a ranked list of the most important features as the explanation. 
The approach is based on the computationally intensive Shapley values \citep{lundberg_unified_2017}, for which the authors develop a tree-specific approximation. 
This differs from our method since identifying the most important features is only a preliminary step in our pipeline --- our work extends beyond this by including
\begin{inparaenum}[(i)]
\item feature bounds that result in reasonable predictions, and 
\item the relationship between the features and the target as a tool to help users inspect what goes wrong when the prediction error is large.
\end{inparaenum}

\citet{ribeiro-2016-should} also propose a method for identifying local feature importances and this is the one we use in our pipeline. 
Their method, LIME, is model-agnostic and is based on approximating the original model locally with a linear model. 
We share their objective of evaluating users' attitudes towards a model through local explanations but we further specify our task as explaining instances where there are large errors in predictions. 
Based on preliminary experiments, we find that LIME is insufficient for our task setting for two reasons: 
\begin{enumerate}[(i)]
\item For regression tasks, LIME's approximation of the original model is not exact. This ``added'' error can be quite large given that our target is typically of order $10^6$, and this convolutes our definition of a large error.
\item The features LIME deems most important are similar regardless of whether the prediction results in a large error or not, which does not provide any specific insight into why a large error occurs. These experiments are detailed in Section~\ref{section:4}.
\end{enumerate}

\noindent%
\subsection{Contrastive Explanations}
Other work on contrastive explanations includes identifying features that should be present or absent in order to justify a classification  \citep{dhurandhar_explanations_2018, ferrari_grounding_2018} or model-agnostic counterfactuals \citep{wachter_counterfactual_2017, russell_efficient_2019}. 
These all differ from our method since they are not specifically about explaining errors. 
Furthermore, the work by \citet{dhurandhar_explanations_2018} and \citet{ferrari_grounding_2018} is based on the binary presence or absence of input features, whereas our method perturbs inputs instead of removing them altogether. 

\subsection{Outlier Detection}

Our work in this chapter can also be viewed as a form of outlier detection. 
However, it differs from the standard literature outlined by \citet{pimentel-2014-review} with respect to the objective: we are not necessarily trying to identify outliers in terms of the training data but rather explain instances in the test set whose errors are so large  that they are considered to be anomalies. 


%\citet{miller_ijcai_2017} perform a survey of the papers cited in the ``Related Works'' section of the call for the IJCAI 2017 Explainable AI workshop \citep{ijcai-2017-workshop} and find that the majority do not base their methods on the available research about explanations from other disciplines such as philosophy, psychology or cognitive sciences, or evaluate on real users. 
%In contrast, our method is rooted in the corresponding philosophical literature \citep{hilton-1990-conversational, lipton-1990-contrastive, hilton-1986-knowledge} and our evaluation is based on a user study. 



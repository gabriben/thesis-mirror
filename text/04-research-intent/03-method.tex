% !TEX root = ../thesis-main.tex

\section{Method: MC-BRP}
\label{section:3}

The intuition behind \OurMethod{} is based on identifying the unusual properties of a particular observation. 
We make the assumption that large errors occur due to unusual feature values in the test set that were not common in the training set. 

Given an observation that results in a large error, \OurMethod{} generates a set of bounds for each feature that would result in a reasonable prediction as opposed to a large error. 
We also include the trend as part of the explanation in order to help users understand the relationship between each feature and the target, and how the input should be changed in order to change the output. 

As pointed out previously, we consider our task of identifying and explaining large errors somewhat similar to that of an outlier detection problem. 
A standard definition of a statistical outlier is an instance that falls outside of a threshold based on the interquartile range. 
A widely used version of this, called Tukey's fences, is defined as follows \citep{tukey-1977-exploratory}:
\[ 
[Q_1 - 1.5(Q_3 - Q_1), Q_3 + 1.5(Q_3 - Q_1)],
\]
where $Q_1$ and $Q_3$ are the first and third quartiles, respectively. 

\begin{definition}\rm
\label{def:large-error}
Let $x$ be an observation in the test set $X$ and let $t$, $\hat{t}$ be the actual and predicted target values of $x$, respectively. Let $\epsilon$ be the corresponding prediction error for $x$, and let $E$ be the set of all errors of $X$. Then $\epsilon$ is a \emph{large error} iff 
\[
\epsilon > Q_3(E) + 1.5(Q_3(E) - Q_1(E)),
\]
where $Q_1(E), Q_3(E)$ are the first and third quartiles of the set of errors, respectively. 
We denote this threshold as $\epsilon_{large}$.
\end{definition}

\noindent%
\if0
We can view $X$ in Definition~\ref{def:large-error} as a disjoint union of two sets, $R$ and $L$, where $R$ is the set of observations that resulted in reasonable predictions, and $L$ is the set of observations that resulted in large errors. 
\fi
We can view $X$ in Definition~\ref{def:large-error} as a disjoint union of two sets:
\begin{enumerate}[(i)]
	\item $R$: the set of observations resulting in reasonable predictions, and
	\item $L$: the set of observations resulting in large errors.
\end{enumerate}
We determine the $n$ most important features based on LIME $\Phi^{(x)} = \{\phi_j^{(x)}\}_{j=1}^{n}$, for all $x \in X$. 
It should be noted there exist alternative methods for determining the most important features for a particular prediction \citep{lundberg_unified_2017}, which would also be appropriate. 



Given $x \in X$, for each $\phi_j^{(x)} \in \Phi^{(x)}$, we determine two sets of characteristics through Monte Carlo simulations:
\begin{enumerate}[(i)]
 \item $[a_{\phi_j^{(x)}}, b_{\phi_j^{(x)}}]$: the bounds for values of $\phi_j^{(x)}$ such that $x \in R$, $x \not\in L$.
 \item $\rho_{\phi_j^{(x)}}$: the relationship between $\phi_j^{(x)}$ and the target, $t$.
\end{enumerate}
We perturb the feature values for $l \in L$ using Monte Carlo simulations in order to determine what feature values are required to produce a reasonable prediction. 
The algorithm for determining $R'$, the set of Monte Carlo simulations resulting in reasonable predictions, is detailed in Algorithm~\ref{mcbrp}.

Given $l \in L$, we determine Tukey's fences for each feature in $\Phi^{(l)}$ based on the feature values from $R$. 
This gives us the bounds from which we sample for our feature perturbations. 

Next, we randomly sample from these bounds for each $\phi_j^{(l)} \in \Phi^{(l)}$ $m$-times to generate $mn$ versions of our original observation, $l$. 
We call the $i$-th perturbed version $l_i'$, where $i \in \left\{1, \ldots, mn\right\}$. 

We then test the original model $f$ on each $l_i'$, obtain a new prediction, $\hat{t_i'}$, and construct $R'$, the set of perturbations resulting in reasonable predictions. 

Once $R'$ is generated, we compute the mean, standard deviation and Pearson coefficient \citep{swinscow-1997-stats} of the top $n$ features of $l \in L$, $\Phi^{(l)}$, based on this set. 
\if0
\bigskip
\begin{algorithm}[h]
    \caption{Monte Carlo simulation: creates a set of perturbed instances resulting in reasonable predictions $R'$ for each large error $l \in L$}
    \label{mcbrp}
    \begin{algorithmic}[1] 
        \Require{instance $l$}
        \Require{set of  $l$'s most important features $\Phi^{(l)}$}
        \Require{`black-box' model $f$}
        \Require{large error threshold $\epsilon_{large}$}
        \Require{number of MC perturbations per feature $m$} 
        \State $R' = \emptyset$ 
            \ForAll{$\phi_j^{(l)}$ in $\Phi^{(l)}$}
                \State $TF(\phi_j^{(l)}$) $\gets$ Tukey's fences for $\phi_j^{(l)}$ \Comment{Based on $R$}
                \For{$i$ in range (0, $m$)}
                    \State $\phi_j'^{(l)} \gets randomsample(TF(\phi_j^{(l)}$))
                    \State $l_i' \gets l_i.replace(\phi_j^{(l)}, \phi_j'^{(l)})$              
                    \State $\hat{t_i'} \gets f(l_i')$ \Comment{New prediction} 
                    \If{$|\hat{t'_i} - t_i| < \epsilon_{large}$}
                        \State {$R' \gets R' \cup l_i'$}
                   %  \EndIf                    
                \EndFor
            \EndFor
            \Return{$R'$}
    \end{algorithmic}
\end{algorithm}
\fi



% MC-BRP
\begin{algorithm}
    \caption{Monte Carlo simulation: creates a set of perturbed instances resulting in reasonable predictions $R'$ for each large error $l \in L$}
    \label{mcbrp}
    \begin{algorithmic}
     \STATE {\bfseries Input:} instance $l$, set of  $l$'s most important features $\Phi^{(l)}$, `black-box' model $f$, large error threshold $\epsilon_{large}$, number of MC perturbations per feature $m$. 
        \STATE $R' = \emptyset$ 	
        
            \FORALL{$\phi_j^{(l)}$ in $\Phi^{(l)}$}
            
                \STATE $TF(\phi_j^{(l)}$) $\gets$ \textsc{Tukeys\_fences}($\phi_j^{(l)}$) \qquad \textcolor{gray}{\textit{\# Based on $R$}} 
                
                \FOR{$i$ in range (0, $m$)}
                
                    \STATE $\phi_j'^{(l)} \gets \textsc{random\_sample}(TF(\phi_j^{(l)}$))
                    
                    \STATE $l_i' \gets l_i.\textsc{replace}(\phi_j^{(l)}, \phi_j'^{(l)})$            
                      
                    \STATE $\hat{t_i'} \gets f(l_i')$ \qquad \textcolor{gray}{\textit{\#New prediction} }
                    
                    \IF{$|\hat{t'_i} - t_i| < \epsilon_{large}$}
                    
                        \STATE {$R' \gets R' \cup l_i'$}
                   \ENDIF                    
                \ENDFOR
            \ENDFOR
            \RETURN{$R'$}
    \end{algorithmic}
\end{algorithm}




\begin{definition}\rm
The \emph{trend}, $\rho_{\phi_j^{(x)}}$, of each feature is the Pearson coefficient between each feature $\phi_j^{(x)}$ and the predictions $\hat{t_i'}$ based on the observations in $R'$. It is a measure of linear correlation between two variables \citep{swinscow-1997-stats}. 
\end{definition}

%noindent%
The set of bounds for each feature in $\Phi^{(x)}$ such that $\hat{t}$ results in a reasonable prediction are based on the mean and standard deviation of each $\phi_j^{(x)} \in \Phi^{(x)}$.

\begin{definition}\rm
The \emph{reasonable bounds} for values of each feature $\phi_j$ in $\Phi^{(x)}$, $[a_{\phi_j^{(x)}}, b_{\phi_j^{(x)}}]$, are 
\[
\left[\mu(\phi_j^{(x)})  - \sigma(\phi_j^{(x)}),    \mu(\phi_j^{(x)})  + \sigma(\phi_j^{(x)})\right],
\]
where $\mu(\phi_j^{(x)})$ and $\sigma(\phi_j^{(x)})$ are the mean and standard deviation of each feature, respectively, based on $R'$. 
\end{definition}

%\noindent%
We compute the trend and the reasonable bounds for each of the $n$ most important features and present them to the user in a table. 
Table~\ref{table:example} shows an example of an explanation generated by \OurMethod{}; the dataset used for this example is detailed in Section~\ref{section:dataset}.


\begin{table}
\caption{An example of an explanation generated by MC-BRP. Here, each of the input values is outside of the range required for a reasonable prediction, which explains why this particular prediction results in a large error. }
\label{table:example}
%\begin{tabular}{cp{5.0cm}p{5.8cm}rc}
\begin{tabular}{cccrc}
\toprule
\bf Input & \bf Definition & \bf Trend & \bf Value & \bf Reasonable\\
\bf  & \bf  & \bf  & \bf  & \bf  range\\
\midrule
A & total\_contract\_hrs & As input $\uparrow$, sales $\uparrow$
 & 9628.0 & [4140,6565] \\
B & advertising\_costs &  As input $\uparrow$, sales $\uparrow$
 & 18160.7 & \phantom{0}[8290,15322] \\
C & num\_transactions & As input $\uparrow$, sales $\uparrow$
 & 97332.0 & [51219,75600] \\
D & total\_headcount & As input $\uparrow$, sales $\uparrow$
 & 226.0 & \phantom{0}[95,153] \\
E & floor\_surface & As input $\uparrow$, sales $\uparrow$
 & 2013.6 & \phantom{0}[972,1725] \\  
\bottomrule
\end{tabular}
\end{table}



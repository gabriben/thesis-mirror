%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Maarten de Rijke at 2019-08-09 19:36:11 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{russell_efficient_2019,
	Abstract = {This paper proposes new search algorithms for counterfactual explanations based upon mixed integer programming. We are concerned with complex data in which variables may take any value from a contiguous range or an additional set of discrete states. We propose a novel set of constraints that we refer to as a "mixed polytope" and show how this can be used with an integer programming solver to efficiently find coherent counterfactual explanations i.e. solutions that are guaranteed to map back onto the underlying data structure, while avoiding the need for brute-force enumeration. We also look at the problem of diverse explanations and show how these can be generated within our framework.},
	Annote = {Comment: FAT* 2019},
	Author = {Russell, Chris},
	Date-Added = {2019-08-05 17:50:29 +0200},
	Date-Modified = {2019-08-09 19:34:32 +0200},
	Journal = {arXiv preprint arXiv:1901.04909},
	Month = jan,
	Title = {Efficient {Search} for {Diverse} {Coherent} {Explanations}},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1901.04909}}

@article{wachter_counterfactual_2017,
	Author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	Date-Added = {2019-08-05 17:49:58 +0200},
	Date-Modified = {2019-08-09 19:36:11 +0200},
	Journal = {SSRN Electronic Journal},
	Title = {Counterfactual {Explanations} {Without} {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	Year = {2017},
	Bdsk-Url-1 = {https://www.ssrn.com/abstract=3063289},
	Bdsk-Url-2 = {https://doi.org/10.2139/ssrn.3063289}}

@incollection{ferrari_grounding_2018,
	Abstract = {Existing visual explanation generating agents learn to fluently justify a class prediction. However, they may mention visual attributes which reflect a strong class prior, although the evidence may not actually be in the image. This is particularly concerning as ultimately such agents fail in building trust with human users. To overcome this limitation, we propose a phrase-critic model to refine generated candidate explanations augmented with flipped phrases which we use as negative examples while training. At inference time, our phrase-critic model takes an image and a candidate explanation as input and outputs a score indicating how well the candidate explanation is grounded in the image. Our explainable AI agent is capable of providing counter arguments for an alternative prediction, i.e. counterfactuals, along with explanations that justify the correct classification decisions. Our model improves the textual explanation quality of fine-grained classification decisions on the CUB dataset by mentioning phrases that are grounded in the image. Moreover, on the FOIL tasks, our agent detects when there is a mistake in the sentence, grounds the incorrect phrase and corrects it significantly better than other models.},
	Address = {Cham},
	Author = {Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
	Booktitle = {Computer {Vision} -- {ECCV} 2018},
	Date-Added = {2019-08-05 17:46:31 +0200},
	Date-Modified = {2019-08-09 19:28:08 +0200},
	Pages = {269--286},
	Publisher = {Springer International Publishing},
	Title = {Grounding {Visual} {Explanations}},
	Urldate = {2019-04-17},
	Volume = {11206},
	Year = {2018},
	Bdsk-Url-1 = {http://link.springer.com/10.1007/978-3-030-01216-8_17},
	Bdsk-Url-2 = {https://doi.org/10.1007/978-3-030-01216-8_17}}

@article{lundberg_explainable_2019,
	Abstract = {Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
	Author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
	Date-Added = {2019-08-05 16:54:21 +0200},
	Date-Modified = {2019-08-09 19:32:17 +0200},
	Journal = {arXiv preprint arXiv:1905.04610},
	Title = {Explainable {AI} for {Trees}: {From} {Local} {Explanations} to {Global} {Understanding}},
	Year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1905.04610}}

@incollection{lundberg_unified_2017,
	Author = {Lundberg, Scott M and Lee, Su-In},
	Booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	Date-Added = {2019-08-05 16:52:30 +0200},
	Date-Modified = {2019-08-09 19:31:42 +0200},
	Pages = {4765--4774},
	Publisher = {Curran Associates, Inc.},
	Title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	Urldate = {2018-10-11},
	Year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}}

@article{van_der_waa_contrastive_2018,
	Abstract = {Recent advances in interpretable Machine Learning (iML) and eXplainable AI (XAI) construct explanations based on the importance of features in classification tasks. However, in a high-dimensional feature space this approach may become unfeasible without restraining the set of important features. We propose to utilize the human tendency to ask questions like "Why this output (the fact) instead of that output (the foil)?" to reduce the number of features to those that play a main role in the asked contrast. Our proposed method utilizes locally trained one-versus-all decision trees to identify the disjoint set of rules that causes the tree to classify data points as the foil and not as the fact. In this study we illustrate this approach on three benchmark classification tasks.},
	Annote = {Comment: presented at 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden},
	Author = {van der Waa, Jasper and Robeer, Marcel and van Diggelen, Jurriaan and Brinkhuis, Matthieu and Neerincx, Mark},
	Date-Added = {2019-08-05 15:43:02 +0200},
	Date-Modified = {2019-08-05 15:43:02 +0200},
	File = {arXiv\:1806.07470 PDF:/Users/alucic/Zotero/storage/EWHYQDLT/van der Waa et al. - 2018 - Contrastive Explanations with Local Foil Trees.pdf:application/pdf;arXiv.org Snapshot:/Users/alucic/Zotero/storage/ZQHXWPMJ/1806.html:text/html},
	Journal = {arXiv:1806.07470 [cs, stat]},
	Keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Machine Learning},
	Month = jun,
	Note = {arXiv: 1806.07470},
	Title = {Contrastive {Explanations} with {Local} {Foil} {Trees}},
	Url = {http://arxiv.org/abs/1806.07470},
	Urldate = {2018-07-26},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1806.07470}}

@inproceedings{tolomei_interpretable_2017,
	Abstract = {Machine-learned models are often described as "black boxes". In many real-world applications however, models may have to sacrifice predictive power in favour of human-interpretability. When this is the case, feature engineering becomes a crucial task, which requires significant and time-consuming human effort. Whilst some features are inherently static, representing properties that cannot be influenced (e.g., the age of an individual), others capture characteristics that could be adjusted (e.g., the daily amount of carbohydrates taken). Nonetheless, once a model is learned from the data, each prediction it makes on new instances is irreversible - assuming every instance to be a static point located in the chosen feature space. There are many circumstances however where it is important to understand (i) why a model outputs a certain prediction on a given instance, (ii) which adjustable features of that instance should be modified, and finally (iii) how to alter such a prediction when the mutated instance is input back to the model. In this paper, we present a technique that exploits the internals of a tree-based ensemble classifier to offer recommendations for transforming true negative instances into positively predicted ones. We demonstrate the validity of our approach using an online advertising application. First, we design a Random Forest classifier that effectively separates between two types of ads: low (negative) and high (positive) quality ads (instances). Then, we introduce an algorithm that provides recommendations that aim to transform a low quality ad (negative instance) into a high quality one (positive instance). Finally, we evaluate our approach on a subset of the active inventory of a large ad network, Yahoo Gemini.},
	Annote = {Comment: 10 pages, KDD 2017},
	Author = {Tolomei, Gabriele and Silvestri, Fabrizio and Haines, Andrew and Lalmas, Mounia},
	Booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '17},
	Date-Added = {2019-08-05 14:15:09 +0200},
	Date-Modified = {2019-08-09 19:35:21 +0200},
	Pages = {465--474},
	Publisher = {ACM},
	Title = {Interpretable {Predictions} of {Tree}-based {Ensembles} via {Actionable} {Feature} {Tweaking}},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1706.06691},
	Bdsk-Url-2 = {https://doi.org/10.1145/3097983.3098039}}

@incollection{dhurandhar_explanations_2018,
	Author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
	Booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	Date-Added = {2019-08-05 13:53:01 +0200},
	Date-Modified = {2019-08-09 19:21:01 +0200},
	Pages = {592--603},
	Publisher = {Curran Associates, Inc.},
	Title = {Explanations based on the {Missing}: {Towards} {Contrastive} {Explanations} with {Pertinent} {Negatives}},
	Year = {2018},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives.pdf}}

@article{pimentel-2014-review,
	Abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as ``one-class classification'', in which a model is constructed to describe ``normal'' training data. The novelty detection approach is typically used when the quantity of available ``abnormal'' data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that ``normality'' may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.},
	Author = {Pimentel, Marco A.F. and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
	Date-Added = {2019-01-10 18:25:01 +0000},
	Date-Modified = {2019-08-09 19:32:58 +0200},
	Journal = {Signal Processing},
	Pages = {215-249},
	Title = {A Review of Novelty Detection},
	Volume = {99},
	Year = {2014},
	Bdsk-Url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S016516841300515X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sigpro.2013.12.026}}

@url{ijcai-2017-workshop,
	Author = {IJCAI},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:30:27 +0200},
	Lastchecked = {28 August 2018},
	Title = {Explainable {AI} Workshop},
	Url = {http://home.earthlink.net/~dwaha/research/meetings/ijcai17-xai/},
	Year = {2017},
	Bdsk-Url-1 = {http://home.earthlink.net/~dwaha/research/meetings/ijcai17-xai/}}

@article{hilton-2017-social,
	Author = {Dennis J. Hilton},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-01-04 18:03:01 +0000},
	Journal = {The Oxford Handbook of Causal Reasoning},
	Title = {Social Attribution and Explanation},
	Year = {2017}}

@article{lipton-1990-contrastive,
	Author = {Peter Lipton},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-01-04 18:03:01 +0000},
	Journal = {Royal Institute of Philosophy Supplement},
	Number = {247-266},
	Title = {Contrastive Explanation},
	Volume = {27},
	Year = {1990}}

@article{hilton-1990-conversational,
	Author = {Dennis J. Hilton},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:29:11 +0200},
	Journal = {Psychological Bulletin},
	Pages = {65--81},
	Title = {Conversational Processes and Causal Explanation},
	Volume = {107},
	Year = {1990}}

@article{hilton-1986-knowledge,
	Author = {Dennis J. Hilton and Ben R. Slugoski},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:28:49 +0200},
	Journal = {Psychological Review},
	Pages = {75-78},
	Title = {Knowledge-based Causal Attribution: The Abnormal Conditions Focus Model},
	Volume = {93},
	Year = {1986}}

@url{fico_2017,
	Author = {FICO},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:26:32 +0200},
	Lastchecked = {15 Aug 2018},
	Title = {Explainable Machine Learning Challenge},
	Url = {https://community.fico.com/s/explainable-machine-learning-challenge},
	Year = {2017},
	Bdsk-Url-1 = {https://community.fico.com/s/explainable-machine-learning-challenge}}

@article{miller_ijcai_2017,
	Abstract = {In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
	Annote = {Comment: IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI)},
	Author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-02-22 22:09:06 +0100},
	Journal = {IJCAI Workshop on Explainable Artificial Intelligence},
	Title = {Explainable {AI}: {Beware} of Inmates Running the Asylum Or: {How} {I} Learnt to Stop Worrying and Love the Social and Behavioural Sciences},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1712.00547}}

@article{dietvorst-2015-aa,
	Author = {Dietvorst, Berkeley J. and Simmons, Joseph P. and Massey, Cade},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:22:30 +0200},
	Journal = {Journal of Experimental Psychology},
	Pages = {114-126},
	Title = {Algorithm Aversion: People Erroneously Avoid Algorithms after Seeing them Err},
	Volume = {144},
	Year = {2015}}

@book{swinscow-1997-stats,
	Author = {Swinscow, Thomas Douglas Victor},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-01-04 18:03:01 +0000},
	Publisher = {BMJ Publishing Group},
	Title = {Statistics at Square One},
	Year = {1997}}

@book{tukey-1977-exploratory,
	Author = {Tukey, John W.},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-01-04 18:03:01 +0000},
	Publisher = {Addison-Wesley},
	Title = {Exploratory Data Analysis},
	Year = {1977}}

@inproceedings{terhoeve-2017-news,
	Author = {ter Hoeve, Maartje and Heruer, Mathieu and Odijk, Daan and Schuth, Anne and Spitters, Martijn and de Rijke, Maarten},
	Booktitle = {FATREC Workshop on Responsible Recommendation},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:29:32 +0200},
	Title = {Do News Consumers Want Explanations for Personalized News Rankings?},
	Year = {2017}}

@article{lakkaraju-2017-interpretable,
	Author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-01-04 18:03:01 +0000},
	Journal = {arXiv preprint arXiv:1707.01154v1},
	Title = {Interpretable and explorable approximations of black box models},
	Year = {2017}}

@article{miller-2017-explanations,
	Author = {Miller, Tim},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:32:47 +0200},
	Journal = {Artificial Intelligence},
	Pages = {1-38},
	Title = {Explanation in Artificial Intelligence: Insights from the Social Sciences},
	Volume = {267},
	Year = {2019}}

@article{khandani-2010-consumer,
	Author = {Khandani, Amir E. and Kim, Adlar J. and Lo, Andrew W.},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:30:54 +0200},
	Journal = {Journal of Banking \& Finance},
	Number = {11},
	Pages = {2767-2787},
	Title = {Consumer Credit-risk Models via Machine Learning Algorithms},
	Volume = {34},
	Year = {2010}}

@misc{propublica,
	Annote = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
	Author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-01-04 18:03:01 +0000},
	Howpublished = {\url{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}},
	Month = {May},
	Title = {Machine Bias: There's software used across the country to predict future criminals. And it's biased against blacks.},
	Year = {2016}}

@article{riberio-2016-model,
	Author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-01-04 18:03:01 +0000},
	Journal = {ICML Workshop on Human Interpretability in Machine Learning},
	Title = {Model-Agnostic Interpretability of Machine Learning},
	Year = {2016}}

@article{doshi-2017-towards,
	Author = {Doshi-Velez, Finale and Kim, Been},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:23:02 +0200},
	Journal = {Explainable and Interpretable Models in Computer Vision and Machine Learning},
	Publisher = {Springer},
	Title = {Considerations for Evaluation and Generalization in Interpretable Machine Learning},
	Year = {2018}}

@article{gdpr,
	Author = {EU},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-02-22 22:09:44 +0100},
	Journal = {Official Journal of the European Union},
	Pages = {1-88},
	Title = {Regulation (EU) 2016/679 of the {European Parliament} and of the {Council} of 27 {April} 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing {Directive 95/46/EC} ({General Data Protection Regulation})},
	Volume = {L119},
	Year = {2016}}

@conference{koh-2017-understanding,
	Author = {Koh, Pang Wei and Liang, Percy},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:31:09 +0200},
	Title = {Understanding Black-box Predictions via Influence Functions},
	Year = {2017}}

@inproceedings{sharchilev-2018-finding,
	Author = {Sharchilev, Boris and Ustinovsky, Yury and Serdyukov, Pavel and de Rijke, Maarten},
	Booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:33:36 +0200},
	Title = {Finding Influential Training Samples for Gradient Boosted Decision Trees},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1802.06640}}

@article{guidotti-2018-survey,
	Author = {Riccardo Guidotti and Anna Monreale and Franco Turini and Dino Pedreschi and Fosca Giannotti},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:27:32 +0200},
	Journal = {ACM {Computing} Surveys},
	Title = {A Survey of Methods for Explaining Black Box Models},
	Volume = {51},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1802.01933}}

@inproceedings{ribeiro-2016-should,
	Author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	Booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	Date-Added = {2019-01-04 18:03:01 +0000},
	Date-Modified = {2019-08-09 19:33:18 +0200},
	Organization = {ACM},
	Pages = {1135-1144},
	Title = {Why Should {I} Trust You?: {Explaining} the Predictions of Any Classifier},
	Year = {2016}}

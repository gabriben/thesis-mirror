% !TEX root = ../thesis-main.tex

\section{Conclusion}
\label{section:7}
In this chapter, we have proposed a method, MC-BRP, that provides users with contrastive explanations about predictions resulting in large errors based on:
\begin{inparaenum}[(i)]
\item the set of bounds for which reasonable predictions would be expected for each of the most important features. 
\item the trend between each of these features and the target.
\end{inparaenum}
%We evaluate \OurMethod{} explanations through a user study with objective and subjective components. 

\noindent%
Given a large error, \OurMethod{} generates a set of perturbed versions of the original instance that result in reasonable predictions. 
This is done by performing Monte Carlo simulations on each of the features deemed most important for the original prediction. 
For each of these features, we determine the bounds needed for a reasonable prediction based on the mean and standard deviation of this new set of reasonable predictions. 
We also determine the relationship between each feature and the target through the Pearson correlation, and present these to the user as the explanation. 

We evaluate \OurMethod{} both objectively (\textbf{RQ3.1}) and subjectively (\textbf{RQ3.2}) by conducting a user study with \numprint{75} real users, including both researchers and practitioners. 
We answer \textbf{RQ3.1} by conducting two types of simulations to quantify how 
\begin{inparaenum}[(i)]
\item interpretable, and 
\item actionable 
\end{inparaenum}
our explanations are. 
Through forward simulations, we show that users are able to interpret \OurMethod{} explanations by simulating the model's output with an average accuracy of 84.5\%. 
Through counterfactual simulations, 
%where users are asked to manipulate the model's input in order to change the output, 
we show that \OurMethod{} explanations are actionable with an accuracy of 76.2\%. 

We answer \textbf{RQ3.2} by conducting a between-subject experiment with subjective questions. 
The treatment group sees \OurMethod{} explanations, while the control group does not see any explanation. 
We find that explanations generated by \OurMethod{} help users understand why models make large errors in predictions (SQ1), but do not have a significant impact on support in deploying the model (SQ2), trust in the model (SQ3), or perceptions of the model's performance (SQ4). 
These results still hold when conditioning on users' background (practitioners vs. researchers). 
We also conduct an analysis on the treatment group to compare results between practitioners and researchers. 
We find significant differences for SQ2, SQ3 and SQ4, but do not find a significant difference in attitudes for SQ1. 

The answer to \textbf{\ref{rq:mcbrp}} is yes: we can create an explanation method based on a real-world use case by first identifying a use case where explanations are required and identifying what users want in the context of this use case. 
We can evaluate in a user-centric manner by conducting a user study based on the use case, which includes both objective and subjective components. 

So far, this thesis has focused on creating knowledge about responsible AI practices, specifically on developing new methods for behavior-based explanations (see Chapter~\ref{chapter:introduction}), where we explain predictions from ML models. 
In the next and final part of the thesis, we will focus more on process-based explanations, where we will investigate how to \emph{translate} knowledge about responsible AI practices to the next generation of AI researchers. 
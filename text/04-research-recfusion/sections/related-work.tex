%!TEX root = ../main.tex

\section{Related Work}
\label{section:focus-relatedwork}
Based on the taxonomy described in Chapter~\ref{chapter:introduction}, our setting in this chapter is a \emph{local explanation} problem for \emph{neural networks}, specifically GNNs. 
We use \emph{sensitivity analysis}, specifically counterfactual perturbations, on \emph{graph} data to generate our explanations. 
Since our work is a counterfactual XAI approach for GNNs, it is related to GNN explainability (Section~\ref{section:rw-gnnxai}) as well as counterfactual explanations (Section~\ref{section:rw-cfexp}). It is also related to adversarial attack methods (Section~\ref{section:rw-adversarial}).

\subsection{GNN Explainability}
\label{section:rw-gnnxai}
Several GNN XAI approaches have been proposed -- a recent survey of the most relevant work is presented by~\citet{yuan2020explainability}.
However, unlike our work, {\em none} of the methods in this survey generate counterfactual explanations. 

The majority of existing GNN XAI methods provide an explanation in the form of a subgraph of the original graph that is deemed to be important for the prediction \citep{yuan2020explainability,baldassarre_explainability_2019,duval2021graphsvx,lin_causal_2021,luo_parameterized_2020,pope_explainability_2019,schlichtkrull_interpreting_2020,vu2020pgmexplainer,ying_gnnexplainer_2019,yuan_subgraph_2021}. We refer to these as \emph{subgraph-generating methods}. 
Such methods are analogous to popular XAI methods such as LIME \citep{ribeiro-2016-should} or SHAP \citep{lundberg_unified_2017}, which identify relevant features for a particular prediction for tabular, image, or text data. 
All of these methods require the user to specify the size of the explanation, $S$, in advance: the number of features (or edges) to keep. 
In contrast, CF-GNNExplainer generates counterfactual explanations, which can find the size of the explanation without requiring input from the user. 
Although both types of techniques are meant for explaining GNN predictions, they are solving fundamentally different problems: counterfactual explanations generate the minimal perturbation such that the prediction changes, while subgraph-retrieving methods identify a relevant (and not necessarily minimal) subgraph that matches the original prediction. 

\pagebreak

The work by \citet{kang_explaine_2019} also generates counterfactual examples for GNNs, but they focus on a different task: link prediction. 
Other GNN XAI methods identify important node features~\citep{huang_graphlime_2020} or similar examples~\citep{faber_contrastive_2020}. 
The works of \citet{yuan_xgnn_2020} and \citet{schnake_xai_2020} generate model-level (i.e., global) explanations for GNNs, which differs from our work since we produce instance-level (i.e., local) explanations. 

\subsection{Counterfactual Explanations}
\label{section:rw-cfexp}
There exists a substantial body of work on counterfactual explanations for tabular, image, and text data \citep{verma2020counterfactual,karimi2020survey,stepin2021survey}.
Some methods treat the underlying classification model as a black-box \citep{laugel_inverse_2017, guidotti_local_2018,lucic_2020_why}, whereas others make use of the model's inner workings \citep{tolomei_interpretable_2017, wachter_counterfactual_2017, ustun_actionable_2019, kanamori_dace_2020, lucic2020focus}.
All of these methods are based on perturbing feature values to generate counterfactual examples -- they are not equipped to handle graph data with relationships (i.e., edges) between instances (i.e., nodes). In contrast, CF-GNNExplainer provides counterfactual examples specifically for graph data. 



\subsection{Adversarial Attacks}
\label{section:rw-adversarial}
Counterfactual examples are also related to adversarial attacks \citep{sun2020adversarial}: they both represent instances obtained from minimal perturbations to the input, which induce changes in the prediction made by the learned model. 
One difference between the two is in the intent: adversarial examples are meant to fool the model, while counterfactual examples are meant to explain the prediction \citep{freiesleben_intriguing_2021,lucic2020focus}. 
In the context of graph data, adversarial attack methods typically make minimal perturbations to the \emph{overall graph} with the intention of degrading overall model performance, as opposed to attacking individual nodes. 
In contrast, we are interested in generating counterfactual examples for individual nodes, as opposed to identifying perturbations to the overall graph. 
We confirm that the counterfactual examples produced by CF-GNNExplainer are informative and not adversarial by measuring the accuracy of our method (see Section~\ref{section:cfgnn-metrics}). 



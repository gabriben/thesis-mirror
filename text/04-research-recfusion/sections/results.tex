%!TEX root = ../main.tex

\section{Results}
\label{section:cfgnn-results}


We evaluate CF-GNNExplainer in terms of the metrics outlined in Section~\ref{section:cfgnn-metrics}. 
The results are shown in Table~\ref{table:results1} and Table~\ref{table:results-gnnexplainer}.  
In cases where the baselines outperform CF-GNNExplainer on a particular metric, they perform poorly on the rest of the metrics, or on other datasets. 




\subsection{Main Findings}

\textbf{Fidelity:}
CF-GNNExplainer outperforms \basekeep{} across all three datasets, and outperforms \baserm{} for \synfour{} and \synfive{} in terms of fidelity. 
We find that \baserand{} has the lowest fidelity in all cases -- it is able to find counterfactual examples for every single node. 
In the following subsections, we will see that this corresponds to poor performance on the other metrics.
\input{04-research-cfgnn/sections/results_table_figure}



\medskip \noindent
\textbf{Explanation Size:}
Figures~\ref{fig:random-explanation-size} to~\ref{fig:explanation-size} show histograms of the explanation size for CF-GNNExplainer and the baselines. 
We see that across all three datasets, CF-GNNExplainer has the smallest (i.e., most minimal) explanation sizes. 
This is especially true when comparing to \baserand{} and \basekeep{} for the \synone{} dataset, where we had to use a different scale for the $x$-axis due to how different the explanation sizes were. 
We postulate that this difference could be because \synone{} is a much more densely connected graph;
it has fewer nodes but more edges compared to the other two datasets, and the average number of nodes and edges in the \cgraph{} is order(s) of magnitude larger (see Table~\ref{table:stats}). 
Therefore, when performing random perturbations, there is substantial opportunity to remove edges that do not necessarily need to be removed, leading to much larger explanation sizes.
When there are many edges in the \cgraph{}, removing everything except the 1-hop neighbourhood, as is done in \basekeep{}, also results in large explanation sizes. 
In contrast, the loss function used by CF-GNNExplainer ensures that only a few edges are removed, which is the desirable behavior since we want minimal explanations. 

\pagebreak

\medskip \noindent
\textbf{Sparsity:}
CF-GNNExplainer outperforms the \baserand{}, \baserm{}, \basekeep{} baselines for all three datasets in terms of sparsity.
%\footnote{GNNExplainer cannot be evaluated on sparsity.} 
We note CF-GNNExplainer and \baserm{} perform much better on this metric in comparison to the other methods, which aligns with the results from explanation size. 

\medskip \noindent
\textbf{Accuracy:}
We observe that CF-GNNExplainer has the highest accuracy for the \synfour{} and \synfive{} datasets, whereas \baserm{} has the highest accuracy for \synone{}. 
However, we are unable to calculate the accuracy of \baserm{} for the other two datasets since it is unable to generate \emph{any} counterfactual examples for motif nodes, contributing to the low sparsity on those datasets. 
We observe accuracy levels upwards of 94\% for CF-GNNExplainer across \emph{all} datasets, indicating that it is consistent in correctly removing edges that are crucial for the initial predictions in the vast majority of cases (see Table~\ref{table:results1}). 






\begin{figure*}[]

    \centering

    \includegraphics[scale=0.27]{04-research-cfgnn/images/tree-cycles-random.png}
    \includegraphics[scale=0.27]{04-research-cfgnn/images/tree-grid-random.png}
    \includegraphics[scale=0.27]{04-research-cfgnn/images/ba-shapes-random.png}
    
        \caption{Histograms showing the proportion of counterfactual examples that have a certain explanation size from \baserand{}. Note the $x$-axis for \synone{} goes up to 1500. Left: \synfour{}, Middle: \synfive{}, Right: \synone{}.  }
        \label{fig:random-explanation-size}
        \bigskip \bigskip
        
    \includegraphics[scale=0.27]{04-research-cfgnn/images/tree-cycles-keep.png}
    \includegraphics[scale=0.27]{04-research-cfgnn/images/tree-grid-keep.png}
    \includegraphics[scale=0.27]{04-research-cfgnn/images/ba-shapes-keep.png}
    
        \caption{Histograms showing the proportion of counterfactual examples that have a certain explanation size from \basekeep{}. Note the $x$-axis for \synone{} goes up to 1500. Left: \synfour{}, Middle: \synfive{}, Right: \synone{}. }
        \label{fig:keep-explanation-size}
        \bigskip \bigskip
        
        
    \includegraphics[scale=0.27]{04-research-cfgnn/images/tree-cycles-remove.png}
    \includegraphics[scale=0.27]{04-research-cfgnn/images/tree-grid-remove.png}
    \includegraphics[scale=0.27]{04-research-cfgnn/images/ba-shapes-remove.png}
    
        \caption{Histograms showing the proportion of counterfactual examples that have a certain explanation size from \baserm{}. Note the $x$-axis for \synone{} goes up to 70. Left: \synfour{}, Middle: \synfive{}, Right: \synone{}. }
        \label{fig:remove-explanation-size}
        \bigskip \bigskip
        
    % \includegraphics[scale=0.38]{images/tree-cycles-gnnexplainer.png}
    % \includegraphics[scale=0.38]{images/tree-grid-gnnexplainer.png}
    % \includegraphics[scale=0.38]{images/ba-shapes-gnnexplainer.png}
    
    %     \caption{Histograms showing explanation size from \gnnexplainer{} for $S=$ GT. Note that the y-axis goes up to 1. Left: \synfour{}, Middle: \synfive{}, Right: \synone{}.}
    %     \label{fig:gnnexplainer-explanation-size}

    \includegraphics[scale=0.27]{04-research-cfgnn/images/tree-cycles.png}
    \includegraphics[scale=0.27]{04-research-cfgnn/images/tree-grid.png}
    \includegraphics[scale=0.27]{04-research-cfgnn/images/ba-shapes.png}
    
        \caption{Histograms showing the proportion of counterfactual examples that have a certain explanation size from CF-GNNExplainer. Note the $x$-axis for \synone{} goes up to 70. Left: \synfour{}, Middle: \synfive{}, Right: \synone{}. }
        \label{fig:explanation-size}
        
\end{figure*}







\subsection{Comparison to \gnnexplainer{}}
Table~\ref{table:results-gnnexplainer} shows the results comparing our method to \gnnexplainer{}. We find that our method outperforms \gnnexplainer{} across all three datasets in terms of both fidelity and accuracy, for all tested values of $S$. 
However, this is not surprising since \gnnexplainer{} is not meant for generating counterfactual explanations, so we cannot expect it to perform well on a task it was not designed for. 
We cannot compare explanation size or sparsity fairly since \gnnexplainer{} requires the user to input the value of $S$. 


\input{04-research-cfgnn/sections/results_gnnexplainer_table}


\subsection{Summary of Results} 
Evaluating on four distinct metrics for each dataset gives us a more holistic view of the results. 
We find that across all three datasets, CF-GNNExplainer can generate counterfactual examples for the majority of nodes in the test set (i.e., low fidelity), while only removing a small number of edges (i.e., low explanation size, high sparsity). For nodes where we know the ground truth (i.e., those in the motifs) we achieve at least 94\% accuracy. 

Although \baserand{} can generate counterfactual examples for every node, they are not very minimal or accurate. 
The latter is also true for \basekeep{} -- in general, it has the worst scores for explanation size, sparsity and accuracy. 
\gnnexplainer{} performs at a similar level as \basekeep{}, indicating that although it is a prominent GNN XAI method, it is not well-suited for solving the counterfactual explanation problem. 

\baserm{} is competitive in terms of explanation size, but it performs poorly in terms of fidelity for the \synfour{} and \synfive{} datasets, and its accuracy on these datasets is unknown since it is unable to generate \emph{any} counterfactual examples for nodes in the motifs. 
These results show that our method is simple and effective in solving the counterfactual explanation task, unlike the baselines we test. 










%!TEX root = ../main.tex

\section{Method: CF-GNNExplainer}
\label{section:cfgnn-method}



To solve the problem defined in Section~\ref{section:problem-formulation}, we propose CF-GNNExplainer, which generates $\bar{v} = (\bar{\compadj}, x)$ given a node $v = (\compadj, x)$.
Our method can operate on any GNN model $f$. 
To illustrate our method and avoid cluttered notation, let $f$ be a standard, one-layer Graph Convolutional Network (GCN)~\citep{kipf_semi_supervised_2017} for node classification:
\begin{align}
    \label{eq:gcn}
    f(A, X; W) = \softmax\left[\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} X W \right], 
\end{align}
where $\tilde{A} = A + I$, $I$ is the identity matrix, $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$ are entries in the degree matrix $\tilde{D}$, $X$ is the node feature matrix, and $W$ is the weight matrix \citep{kipf_semi_supervised_2017}. 







\subsection{Adjacency Matrix Perturbation}
\label{section:adjacency-perturb}
First, we define $\bar{\compadj} = \perturbb \odot \compadj$, where $\perturbb$ is a binary perturbation matrix that sparsifies $\compadj$. 
Our aim is to find $\perturbb$ for a given node $v$ such that $f(\compadj, x) \neq f(\perturbb \odot \compadj, x$). 
To find $\perturbb$, we build upon the method by \citet{srinivas_training_2016} for training sparse neural networks (see Section~\ref{section:matrix-sparsification}), where our objective is to zero out entries in the adjacency matrix (i.e., remove edges).
That is, we want to find $\perturbb$ that minimally perturbs $\compadj$, and use it to compute $\bar{\compadj} = \perturbb \odot \compadj$. 
If an element $\perturbb_{{i,j}} = 0$, this results in the deletion of the edge between node $i$ and node $j$. 
When $\perturbb$ is a matrix of ones, this indicates that all edges in $\compadj$ are used in the forward pass. 

Similar to the work by \citet{srinivas_training_2016}, we first generate an intermediate, real-valued matrix $\perturbl$ with entries in $\left[0, 1\right]$, apply a sigmoid transformation, then threshold the entries to arrive at a binary $\perturbb$: entries greater than or equal to 0.5 become 1, while those below 0.5 become 0. 
In the case of undirected graphs (i.e., those with symmetric adjacency matrices), we first generate a perturbation vector, which we then use to populate $\perturbl$ in a symmetric manner, instead of generating $\perturbl$ directly. 

\pagebreak


\subsection{Counterfactual Generating Model}
We want our perturbation matrix $\perturbb$ to only act on $\compadj$, not $\tilde{\compadj}$, in order to preserve self-loops in the message passing of $f$. This is because we always want a node representation update to include its own representation from the previous layer. 
Therefore we first rewrite Equation~\ref{eq:gcn} for our illustrative one-layer case to isolate $\compadj$: 
%
\begin{align}
f(\compadj, \compnode; W) = \softmax\left[(\compdeg + I)^{-1/2} (\compadj + I) (\compdeg + I)^{-1/2} \compnode W\right].
    \label{eq:gcn3}
\end{align}
%
To generate CFs, we propose a new function $g$, which is based on $f$, but it is parameterized by $\perturbb$ instead of $W$. 
We update the degree matrix $\compdeg$ based on $\perturbb \odot \compadj$, add the identity matrix to account for self-loops (as in $\tilde{\compdeg}$ in Equation~\ref{eq:gcn}), and call this $\bar{\compdeg}$: 
%
\begin{align}
    \label{eq:cf}
    g(\compadj, \compnode, W; \perturbb) = \softmax\left[\bar{\compdeg}^{-1/2} (\perturbb \odot \compadj + I) \bar{\compdeg}^{-1/2} \compnode W\right].
\end{align}
%
In other words, $f$ learns the weight matrix while holding the data constant, while $g$ 
%is optimized to find a perturbation matrix that is then used to 
generates new data points (i.e., counterfactual examples) while holding the weight matrix (i.e., model) constant. 
Another distinction between $f$ and $g$ is that the aim of $f$ is to find the optimal set of weights that generalizes well on an unseen test set, while the objective of $g$ is to generate an optimal counterfactual example, given a particular node (i.e., $\bar{v}$ is the output of $g$). 


\subsection{Loss Function Optimization}
We generate $\perturbb$ by minimizing Equation~\ref{eq:loss-graph}, adopting the negative log-likelihood (NLL) loss  for $\losspred$:
\begin{align}
    &\losspred(v, \bar{v}|f, g) =\mathbbm{1}\left[f(v) = f(\bar{v})\right] \cdot \mathcal{L}_{NLL}(f(v), g(\bar{v})).
    \label{eq:loss-pred}
\end{align}
Since we do not want $f(\bar{v})$ to match $f(v)$, we put a negative sign in front of $\losspred$ and include an indicator function to ensure the loss is active as long as $f(\bar{v}) = f(v)$. 
Note that $f$ and $g$ have the same weight matrix $W$ -- the main difference is that $g$ also includes the perturbation matrix $\perturbb$. 


$\lossdist$ can be based on any differentiable distance function. 
In our case, we take $d$ to be the element-wise difference between $v$ and $\bar{v}$, corresponding to the difference between $\compadj$ and $\bar{\compadj}$: the number of edges removed. 
For undirected graphs, we divide this value by 2 to account for the symmetry in the adjacency matrices. 
When updating $\perturbb$, we take the gradient of Equation~\ref{eq:loss-graph} with respect to the intermediate $\perturbl$, \emph{not} the binary $\perturbb$. 

\pagebreak

\begin{algorithm}[t]
    \caption{CF-GNNExplainer: given a node $v = (\compadj, x)$ where $f(v) = y$, generate the minimal perturbation, $\bar{v} = (\bar{\compadj}, x)$, such that $f(\bar{v}) \neq y$.}
    
    \label{alg:cf-gnnexplainer}

    \begin{algorithmic}
        \STATE {\bfseries Input:} node $v = (\compadj, x)$, trained GNN model $f$, CF model $g$, loss function $\loss$, learning rate $\alpha$, number of iterations $K$, distance function $d$. 
        
        \STATE
        
        % \STATE $y$ = \textsc{get\_gnn\_prediction($v$)} 
        \STATE $f(v) = y$ \qquad \textcolor{gray}{\textit{\# Get GNN prediction} } 
        
        % \STATE $\compadj, \compdeg \gets normalize(v)$

        
        % \STATE \textcolor{gray}{\textit{\# Initialization} }

        \STATE $\perturbl \gets J_n$ \qquad  \textcolor{gray}{\textit{\# Initialization} } 
        
        \STATE $\bar{v}^* = \left[ \:\right]$
        % \STATE $\bar{\compgraph} \gets \perturbl \odot \compadj$ 
        % \STATE $\bar{\compgraph} \gets \bar{\compgraph}$
        
        \STATE
        
        \FOR{$K$ iterations} \label{line:start_loop}
            \STATE $\bar{v}$ = \textsc{get\_cf\_example()}
            %\STATE \textsc{compute\_loss($v$, $\bar{v}$)}
            \STATE $\loss \gets \loss(v, \bar{v}, f, g)$ \qquad \textcolor{gray}{\textit{\# Eq~\ref{eq:loss-graph} \& ~\ref{eq:loss-pred}} } 
            %\STATE \textsc{update\_p()}
            \STATE $\perturbl \gets \perturbl + \alpha \nabla_{\perturbl} \loss$ \qquad \textcolor{gray}{\textit{\# Update $\perturbl$}} 
        \ENDFOR
        
        \STATE

        \STATE \textbf{Function} \textsc{get\_cf\_example()}
        
       \bindent
        
        \STATE $\perturbb \gets \text{threshold}(\sigma(\perturbl))$
        
        \STATE $\bar{\compadj} \gets \perturbb \odot \compadj$
        \STATE $\bar{v}_{cand} \gets (\bar{\compadj}, x)$
        
        \IF{$f(v) \neq f(\bar{v}_{cand})$} 
            \STATE $\bar{v} \gets \bar{v}_{cand}$
            
            \IF{not $\bar{v}^*$}
            	\STATE $\bar{v}^* \gets \bar{v}$ \qquad \textcolor{gray}{\textit{\# First CF}} 
            
            \ELSIF{$d(v, \bar{v}) \leq d(v, \bar{v}^*)$}
            
                \STATE $\bar{v}^* \gets \bar{v}$ \qquad \textcolor{gray}{\textit{\# Best CF}} 
            \ENDIF
                    
        \ENDIF
        
        \RETURN{$\bar{v}^*$}
        
        \eindent
        
        % \STATE
        
        % \RETURN $\bar{v}^*$
        
    \end{algorithmic}
\end{algorithm}

\subsection{CF-GNNExplainer }

We call our method CF-GNNExplainer and summarize its details in Algorithm~\ref{alg:cf-gnnexplainer}. 
Given an node in the test set $v$, we first obtain its original prediction from $f$ and initialize ${\perturbl}$ as a matrix of ones, $J_n$, to initially retain all edges. 
Next, we run CF-GNNExplainer for $K$ iterations.  
To find a counterfactual example, we use Equation~\ref{eq:cf}. 

First, we compute $\perturbb$ by thresholding $\perturbl$ (see Section~\ref{section:adjacency-perturb}). 
Then we use $\perturbb$ to obtain the sparsified adjacency matrix that gives us a candidate counterfactual example, $\bar{v}_{cand}$. 
This example is then fed to the original GNN, $f$, and if $f$ predicts a different output than for the original node, we have found a valid counterfactual example, $\bar{v}$. 

We keep track of the ``best'' counterfactual example (i.e., the most minimal according to $d$), and return this as the optimal counterfactual example $\bar{v}^*$ after $K$ iterations. 
Between iterations, we compute the loss following Equations~\ref{eq:loss-graph} and \ref{eq:loss-pred}, and update $\perturbl$ based on the gradient of the loss. 
In the end, we retrieve the optimal counterfactual explanation $\Delta_v^* = v - \bar{v}^* $. 

\subsection{Complexity}
\label{section:complexity}
CF-GNNExplainer has time complexity $O(KN^2)$, where $N$ is the number of nodes in the subgraph neighbourhood and $K$ is the number of iterations. We note that high complexity is common for local XAI methods (i.e., SHAP \citep{lundberg_unified_2017}, GNNExplainer \citep{ying_gnnexplainer_2019}, etc.), but in practice, one typically only generates explanations for a subset of the dataset. 



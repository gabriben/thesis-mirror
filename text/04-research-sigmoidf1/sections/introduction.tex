%!TEX root = ../main.tex

\section{Introduction}
\label{section:focus-intro}
As ML models are prominently applied and their outcomes have a substantial effect on the general population, there is an increased demand for understanding what contributes to their predictions \citep{doshi-2017-towards}. 
For an individual who is affected by the predictions of these models, it would be useful to have an \emph{actionable} explanation -- one that provides insight into how these decisions can be \emph{changed}. 
The General Data Protection Regulation (GDPR) is an example of recently enforced regulation in Europe which gives an individual the right to an explanation for algorithmic decisions, making the interpretability problem a crucial one for organizations that wish to adopt more data-driven decision-making processes \citep{gdpr}. 

Counterfactual explanations are a natural solution to this problem since they frame the explanation in terms of what input (feature) changes are required to change the output (prediction). 
For instance, a user may be denied a loan based on the prediction of an ML model used by their bank. 
A counterfactual explanation could be: ``\textit{Had your income been \euro  $1000$ higher, you would have been approved for the loan}.''
We focus on finding \emph{optimal} counterfactual explanations: the \emph{minimal} changes to the input required to change the outcome. 

Counterfactual explanations are based on \emph{counterfactual examples}: generated instances that are close to an existing instance but have an alternative prediction. 
The difference between the original instance and the counterfactual example is the counterfactual explanation. 
\citet{wachter_counterfactual_2017} propose framing the problem as an optimization task, but their work assumes that the underlying machine learning models are differentiable, which excludes an important class of widely applied and highly effective non-differentiable models: tree ensembles. 
We propose a method that relaxes this assumption and builds upon the work of \citeauthor{wachter_counterfactual_2017} by introducing differentiable approximations of tree ensembles that can be used in such an optimization framework. 
Alternative non-optimization approaches for generating counterfactual explanations for tree ensembles involve an extensive search over many possible paths in the ensemble that could lead to an alternative prediction \citep{tolomei_interpretable_2017}. 

Given a trained tree-based model $f$, we probabilistically approximate $f$ by replacing each split in each tree with a sigmoid function centred at the splitting threshold. If $f$ is an ensemble of trees, then we also replace the maximum operator with a softmax. 
This approximation allows us to generate a counterfactual example $\bar{x}$ for an instance $x$ based on the minimal perturbation of $x$ such that the prediction changes: $y_{x} \neq y_{\bar{x}}$, where $y_{x}$ and $y_{\bar{x}}$ are the labels $f$ assigns to $x$ and $\bar{x}$, respectively. 
This leads us to our main research question in this chapter:
%
\begin{quote}
\emph{Are counterfactual examples generated by our method closer to the original input instances than those generated by existing heuristic methods?}
\end{quote}
%
Our main findings are that our proposed method is
\begin{enumerate*}[label=(\roman*)]
\item a more \emph{effective} counterfactual explanation method for tree ensembles than previous approaches since it manages to produce counterfactual examples that are closer to the original input instances than existing approaches; 
\item a more \emph{efficient} counterfactual explanation method for tree ensembles since it is able to handle larger models than existing approaches; and
\item a more \emph{reliable} counterfactual explanation method for tree ensembles since it is able to generate counterfactual explanations for all instances in a dataset, unlike existing approaches specific to tree ensembles. 
\end{enumerate*}

In the following sections, we examine existing work related to ours (Section~\ref{section:focus-related-work}) and formalize the counterfactual explanation problem (Section~\ref{section:focus-problem-definition}). 
We then describe the details of our method, Flexible Optimizable CoUnterfactual Explanations for Tree EnsembleS (FOCUS), in Section~\ref{section:focus-method}. 
In Section~\ref{section:focus-exp-setup}, we explain the experimental setup, followed by the experimental results in Sections~\ref{section:experiment1} and~\ref{section:experiment2}. 
We analyze our findings in Section~\ref{section:case-study} and conclude in Section~\ref{section:focus-conclusion}. 
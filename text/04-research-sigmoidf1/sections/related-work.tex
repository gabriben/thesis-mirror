%!TEX root = ../main.tex

\section{Related Work}
\label{section:focus-related-work}
Based on the taxonomy described in Chapter~\ref{chapter:introduction}, our setting in this chapter is a \emph{local explanation} problem for \emph{tree ensembles}. 
We use \emph{sensitivity analysis}, specifically counterfactual perturbations, on \emph{tabular} data to generate our explanations. 
Our work is related to counterfactual explanations in general (Section~\ref{section:focus-cf}), algorithmic recourse (Section~\ref{section:focus-recourse}), adversarial examples (Section~\ref{section:focus-adversarial}), and differentiable tree-based models (Section~\ref{section:focus-diff-trees}).

\subsection{Counterfactual Explanations}
\label{section:focus-cf}
Counterfactual examples have been used in a variety of ML areas, such as reinforcement learning \citep{madumal_explainable_2019}, deep learning \citep{alaa_deep_2017}, and XAI. 
Previous XAI methods for generating counterfactual examples are either model-agnostic \citep{poyiadzi_face_2020, karimi_model-agnostic_2019, laugel_inverse_2017, van_looveren_interpretable_2020,  mothilal_explaining_2020} or model-specific \citep{wachter_counterfactual_2017, grath_interpretable_2018, tolomei_interpretable_2017, kanamori_dace_2020, russell_efficient_2019, dhurandhar_explanations_2018}. 
Model-agnostic approaches treat the original model as a ``black-box'' and only assume query access to the model, whereas model-specific approaches typically do not make this assumption and can therefore make use of its inner workings (see Chapter~\ref{chapter:introduction}). 

Our work is a model-specific approach for generating counterfactual examples through optimization. 
Previous model-specific work for generating counterfactual examples through optimization has solely been conducted on differentiable models \citep{wachter_counterfactual_2017, grath_interpretable_2018, dhurandhar_explanations_2018}. 

\subsection{Algorithmic Recourse}
\label{section:focus-recourse}
Algorithmic recourse is a line of research that is closely related to counterfactual explanations, except that methods for algorithmic recourse include the additional restriction that the resulting explanation must be \emph{actionable} \citep{ustun_actionable_2019, joshi_towards_2019, karimi_recourse_2020, karimi_imperfect_causal_2020}. 
This is done by selecting a subset of the features to which perturbations can be applied in order to avoid explanations that suggest impossible or unrealistic changes to the feature values (i.e., change \textit{age} from \numprint{50} $\to$ \numprint{25}). 
Although this work has produced impressive theoretical results, it is unclear how realistic they are in practice, especially for complex ML models such as tree ensembles. 
Existing algorithmic recourse methods cannot solve our task because they 
\begin{enumerate*}[label=(\roman*)]
	\item are either restricted to solely linear \citep{ustun_actionable_2019} or  differentiable \citep{joshi_towards_2019} models, or
	\item  require access to causal information \citep{karimi_recourse_2020, karimi_imperfect_causal_2020}, which is rarely available in real world settings. 
\end{enumerate*}

\subsection{Adversarial Examples}
\label{section:focus-adversarial}
Adversarial examples are a type of counterfactual example with the additional constraint that the minimal perturbation results in an alternative prediction that is \emph{incorrect}. 
There are a variety of methods for generating adversarial examples \citep{goodfellow_explaining_2015,szegedy_intriguing_2014,su_one_2019,brown_adversarial_2018}; a more complete overview can be found in the work of \cite{biggio_wild_2018}. 
The main difference between adversarial examples and counterfactual examples is in the intent: adversarial examples are meant to \emph{fool} the model, whereas counterfactual examples are meant to \emph{explain} the model.


\subsection{Differentiable Tree-based Models}
\label{section:focus-diff-trees}
Part of our contribution involves constructing differentiable versions of tree ensembles by replacing each splitting threshold with a sigmoid function. 
This can be seen as using a (small) neural network to obtain a smooth approximation of each tree. 
Neural decision trees \citep{balestriero_neural_2017, yang_deep_2018} are also differentiable versions of trees, which use a full neural network instead of a simple sigmoid. 
However, these do not optimize for approximating an already trained model. Therefore, unlike our method, they are not an obvious choice for finding counterfactual examples for an existing model. 
Soft decision trees~\citep{hinton_distilling_2014} are another example of differentiable trees, which instead approximate a neural network with a decision tree. 
This can be seen as the inverse of our task. 

